{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "volcan_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpv6oO-K1pq8"
      },
      "source": [
        "If you want to work on colab, you would need to install Optuna (for hyper-parametter tunning) and also to update XGBoost package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWWnLMIkMa3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f04a6f3-ac60-4d1c-aa5f-d380d7a862cc"
      },
      "source": [
        "!pip install -U xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/cc/fd3d5fc6b6616a03385a0f6492cc77a253940d1026406ecc07597095e381/xgboost-1.2.1-py3-none-manylinux2010_x86_64.whl (148.9MB)\n",
            "\u001b[K     |████████████████████████████████| 148.9MB 78kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Installing collected packages: xgboost\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbTTEEL87g_F"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scmDhhyR1NMX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import xgboost\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmRMdnpx1CzD"
      },
      "source": [
        "I processed the sesors signal and extracted some features from that, including FTT signal, mean, median, quantile and etc. All of then are saved in `out.csv` file which I am going to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWcrKO761yIQ"
      },
      "source": [
        "path='drive/My Drive/Soheil/volcan/new3/'\n",
        "train_df=pd.read_csv(path+'out.csv',).to_numpy()\n",
        "X=train_df[:,1:-1]\n",
        "y=train_df[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFFn2q-t19T0"
      },
      "source": [
        "Some of my features are useless as their variances is zeros. I am going to remove them along with features with correlation more than 0.99"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSbz-sHvDmM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c4713a-8f33-432c-ccf4-37e9caf18be7"
      },
      "source": [
        "sel = VarianceThreshold(threshold=0.0)\n",
        "sel.fit(X)\n",
        "drop_var=np.where(sel.variances_==0)[0]\n",
        "corr_mat=np.corrcoef(X.T)\n",
        "cols_=[]\n",
        "for i in range(1,corr_mat.shape[1]):\n",
        "  for j in range(i):\n",
        "    if corr_mat[i,j]>0.99:\n",
        "      cols_.append(i)\n",
        "all_drop=np.unique(np.hstack((drop_corr,drop_var)))\n",
        "X_=np.delete(X,all_drop,1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_variance_threshold.py:77: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  self.variances_ = np.nanvar(X, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_variance_threshold.py:85: RuntimeWarning: All-NaN slice encountered\n",
            "  self.variances_ = np.nanmin(compare_arr, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_variance_threshold.py:88: RuntimeWarning: invalid value encountered in less_equal\n",
            "  (self.variances_ <= self.threshold)):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VarianceThreshold(threshold=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bb_iJlz2lt0"
      },
      "source": [
        "To have similar distribution in my folds by using KFold, I am going to sectionize target and then to use StraifiedKfold instead of Kfold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jUIOkcFBVjg"
      },
      "source": [
        "def reg2class(series_):\n",
        "\n",
        "    count, division = np.histogram(series_,bins=50)\n",
        "    min_count=np.min(count)\n",
        "    class_=[]\n",
        "    for j_ in range(series_.shape[0]):\n",
        "        for i_ in range(division.shape[0]-1):\n",
        "            if series_.iloc[j_]<=division[i_+1] and series_.iloc[j_]>=division[i_]:\n",
        "                class_.append(i_)\n",
        "    return np.array(pd.DataFrame(class_)[0]),min_count\n",
        "y_class,max_fold_=reg2class(pd.DataFrame(y[:,None])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2UR8fp33uwc"
      },
      "source": [
        "In below cell, I am going to find the most important features by using XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzKSDDEWCsSG"
      },
      "source": [
        "\n",
        "n_folds=10\n",
        "importance_=np.zeros((X_.shape[1],n_folds))\n",
        "mae=[]\n",
        "for i,(tr_,ts_) in enumerate(StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=1370).split(X_,y_class)) :\n",
        "    model1 = xgboost.XGBRegressor(n_estimators=3000,tree_method='gpu_hist',max_depth=10,\n",
        "                              learning_rate=0.005,\n",
        "                              min_child_weight=7,\n",
        "                              eta= 0.005,\n",
        "                              subsample=0.8, \n",
        "                              colsample_bytree=0.7, \n",
        "                              reg_alpha=1e-05,\n",
        "                              gamma=0.4)\n",
        "\n",
        "    eval_set = [(X_[ts_,:], y[ts_])]\n",
        "    model1.fit(X_[tr_,:], y[tr_],early_stopping_rounds=5,eval_metric='rmse', eval_set=eval_set, verbose=1)\n",
        "    importance_[:,i]=model1.feature_importances_\n",
        "    mae_=model1.best_score\n",
        "    print(mae_)\n",
        "    mae.append(mae_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "755LICdM4L8T"
      },
      "source": [
        "I dropped features with scores less than quantile 0.75\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5_nyauMKYOo"
      },
      "source": [
        "val_=np.mean(importance_,axis=1)\n",
        "drop_imp=np.where(val_<np.quantile(val_,[0.75])[0])[0]\n",
        "X__=np.delete(X_,drop_imp,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhFFmam_G5H7"
      },
      "source": [
        "test_df=pd.read_csv(path+'out_test.csv').to_numpy()\n",
        "X_test=test_df[:,1:]\n",
        "X_test_=np.delete(X_test,all_drop,1)\n",
        "X_test__=np.delete(X_test_,drop_imp,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeaePc4EX2TV"
      },
      "source": [
        "X_test__.shape,X__.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "589H99rQ5twk"
      },
      "source": [
        "Hyper-parameter tuning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s1n5oJyNjl9"
      },
      "source": [
        "def objective(trial):\n",
        "\n",
        "\n",
        "  n_folds=10\n",
        "  mae=[]\n",
        "  for i,(tr_,ts_) in enumerate(StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=1370).split(X__,y_class)) :\n",
        "\n",
        "    model1 = xgboost.XGBRegressor(n_estimators=3000,tree_method='gpu_hist',\n",
        "                                    gamma=trial.suggest_loguniform(\"gamma\", 0.3, 0.5),\n",
        "                                    min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 20),\n",
        "                                    max_depth=trial.suggest_int(\"max_depth\", 3, 20),\n",
        "                                    learning_rate=trial.suggest_loguniform(\"learning_rate\", 0.0005, 0.5),\n",
        "                                    eta=trial.suggest_loguniform(\"eta\",  0.0005, 0.5),\n",
        "                                    subsample=trial.suggest_loguniform(\"subsample\", 0.1,1),\n",
        "                                    colsample_bytree=trial.suggest_loguniform(\"colsample_bytree\", 0.1, 1),\n",
        "                                    reg_alpha=trial.suggest_loguniform(\"reg_alpha\", 0.001, 50))\n",
        "\n",
        "    eval_set = [(X__[ts_,:], y[ts_])]\n",
        "    model1.fit(X__[tr_,:], y[tr_],early_stopping_rounds=5,eval_metric='mae', eval_set=eval_set, verbose=False)\n",
        "\n",
        "    mae_=model1.best_score\n",
        "    print(f'{mae_} {i}')\n",
        "    mae.append(mae_)\n",
        "\n",
        "  return np.mean(np.array(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxg-KZQOEJwN"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(objective,n_trials=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGlIvzjI5z2O"
      },
      "source": [
        "using optimum hyper-parametter, doing prediction and submiting the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NcQS9TgjX2u"
      },
      "source": [
        "n_folds=10\n",
        "repeat_=3\n",
        "res=np.zeros((X_test__.shape[0],n_folds*repeat_))\n",
        "mae=[]\n",
        "for j in range(repeat_):\n",
        "  for i,(tr_,ts_) in enumerate(StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=j**2).split(X__,y_class)) :\n",
        "    model1 = xgboost.XGBRegressor(n_estimators=3000,tree_method='gpu_hist',max_depth=19,eta=0.009747282892152175,\n",
        "                                learning_rate=0.002369858098148533,\n",
        "                                gamma=0.39999429394579983,subsample=0.7990451509767214,colsample_bytree= 0.3476157291201921,reg_alpha=1.7168175901019114,min_child_weight= 5)\n",
        "    \n",
        "    eval_set = [(X__[ts_,:], y[ts_])]\n",
        "    model1.fit(X__[tr_,:], y[tr_],early_stopping_rounds=5,eval_metric='mae', eval_set=eval_set, verbose=False)\n",
        "\n",
        "    mae_=model1.best_score\n",
        "    print(mae_)\n",
        "    mae.append(mae_)\n",
        "    res[:,i+j*n_folds]=model1.predict(X_test__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvtD6GqgtFR2"
      },
      "source": [
        "sample_submission_df=pd.read_csv(path+'sample_submission.csv')\n",
        "sample_submission_df['time_to_eruption']=res.median(axis=1)[:,None]\n",
        "sample_submission_df.to_csv('7folds_optimized_dim_red4.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}